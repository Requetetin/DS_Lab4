{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import numpy as np\r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.util import ngrams\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from collections import defaultdict\r\n",
    "from collections import  Counter\r\n",
    "plt.style.use('ggplot')\r\n",
    "stop=set(stopwords.words('english'))\r\n",
    "import re\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import gensim\r\n",
    "import string\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras_preprocessing.sequence import pad_sequences\r\n",
    "from tqdm import tqdm\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\r\n",
    "from keras.initializers import Constant\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\r\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.target.value_counts()\r\n",
    "sns.barplot(x.index, x)\r\n",
    "plt.gca().set_ylabel('samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen mas tweets que no hacen referencia a desastre (0) que aquellos que si hacen referencia a un desastre (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de caracteres en un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\r\n",
    "tweet_length = train[train['target']==1]['text'].str.len()\r\n",
    "ax1.hist(tweet_length, color='red')\r\n",
    "ax1.set_title('desastres')\r\n",
    "tweet_length=train[train['target']==0]['text'].str.len()\r\n",
    "ax2.hist(tweet_length, color='blue')\r\n",
    "ax2.set_title('no desastres')\r\n",
    "fig.suptitle('Cantidad de caracteres del tweet')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de palabras en el tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\r\n",
    "tweet_length=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\r\n",
    "ax1.hist(tweet_length,color='red')\r\n",
    "ax1.set_title('desastres')\r\n",
    "tweet_length=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\r\n",
    "ax2.hist(tweet_length,color='blue')\r\n",
    "ax2.set_title('no desastres')\r\n",
    "fig.suptitle('Cantidad de palabras')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promedio de longitud de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\r\n",
    "word=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\r\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\r\n",
    "ax1.set_title('desastres')\r\n",
    "word=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\r\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='blue')\r\n",
    "ax2.set_title('no desastres')\r\n",
    "fig.suptitle('Promedio de largo de las palabras en los tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(target):\r\n",
    "    corpus=[]\r\n",
    "    \r\n",
    "    for x in train[train['target']==target]['text'].str.split():\r\n",
    "        for i in x:\r\n",
    "            corpus.append(i)\r\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabra vacias (stopwords) en los tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para no desastres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus(0)\r\n",
    "\r\n",
    "dic=defaultdict(int)\r\n",
    "for word in corpus:\r\n",
    "    if word in stop:\r\n",
    "        dic[word]+=1\r\n",
    "        \r\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=zip(*top)\r\n",
    "plt.bar(x,y, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets de desastres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus(1)\r\n",
    "\r\n",
    "dic=defaultdict(int)\r\n",
    "for word in corpus:\r\n",
    "    if word in stop:\r\n",
    "        dic[word]+=1\r\n",
    "\r\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=zip(*top)\r\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de puntuacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desastres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\r\n",
    "corpus=create_corpus(1)\r\n",
    "\r\n",
    "dic=defaultdict(int)\r\n",
    "import string\r\n",
    "special = string.punctuation\r\n",
    "for i in (corpus):\r\n",
    "    if i in special:\r\n",
    "        dic[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=zip(*dic.items())\r\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No desastres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\r\n",
    "corpus=create_corpus(0)\r\n",
    "\r\n",
    "dic=defaultdict(int)\r\n",
    "import string\r\n",
    "special = string.punctuation\r\n",
    "for i in (corpus):\r\n",
    "    if i in special:\r\n",
    "        dic[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=zip(*dic.items())\r\n",
    "plt.bar(x,y,color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=Counter(corpus)\r\n",
    "most=counter.most_common()\r\n",
    "x=[]\r\n",
    "y=[]\r\n",
    "for word,count in most[:40]:\r\n",
    "    if (word not in stop) :\r\n",
    "        x.append(word)\r\n",
    "        y.append(count)\r\n",
    "\r\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tweet_unigrams(corpus, n=None):\r\n",
    "    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\r\n",
    "    bag_of_words = vec.transform(corpus)\r\n",
    "    sum_words = bag_of_words.sum(axis=0) \r\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\r\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\r\n",
    "top_tweet_unigrams=get_top_tweet_unigrams(train['text'])[:10]\r\n",
    "x,y=map(list,zip(*top_tweet_unigrams))\r\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrama (n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\r\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\r\n",
    "    bag_of_words = vec.transform(corpus)\r\n",
    "    sum_words = bag_of_words.sum(axis=0) \r\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\r\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\r\n",
    "top_tweet_bigrams=get_top_tweet_bigrams(train['text'])[:10]\r\n",
    "x,y=map(list,zip(*top_tweet_bigrams))\r\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tweet_trigrams(corpus, n=None):\r\n",
    "    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\r\n",
    "    bag_of_words = vec.transform(corpus)\r\n",
    "    sum_words = bag_of_words.sum(axis=0) \r\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\r\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\r\n",
    "top_tweet_trigrams=get_top_tweet_trigrams(train['text'])[:10]\r\n",
    "x,y=map(list,zip(*top_tweet_trigrams))\r\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n = 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tweet_ngrams(corpus, n=None):\r\n",
    "    vec = CountVectorizer(ngram_range=(1, 5)).fit(corpus)\r\n",
    "    bag_of_words = vec.transform(corpus)\r\n",
    "    sum_words = bag_of_words.sum(axis=0) \r\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\r\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\r\n",
    "top_tweet_ngrams=get_top_tweet_ngrams(train['text'])[:10]\r\n",
    "x,y=map(list,zip(*top_tweet_ngrams))\r\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitar URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\r\n",
    "  url = re.compile(r'https?://\\S+|www\\.\\S+')\r\n",
    "  return url.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitar tags HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_HTML(text):\r\n",
    "  html = re.compile(r'<.*?>')\r\n",
    "  return html.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x : remove_HTML(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitar emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\r\n",
    "    emoji_pattern = re.compile(\"[\"\r\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
    "                           u\"\\U00002702-\\U000027B0\"\r\n",
    "                           u\"\\U000024C2-\\U0001F251\"\r\n",
    "                           \"]+\", flags=re.UNICODE)\r\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x : remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitar puntuacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_point(text):\r\n",
    "  table = str.maketrans('', '', string.punctuation)\r\n",
    "  return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x : remove_point(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corregir errores de ortografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\r\n",
    "\r\n",
    "spell = SpellChecker()\r\n",
    "def correct_spelling(text):\r\n",
    "  corrected_str = []\r\n",
    "  misspelled_str = spell.unknown(text.split())\r\n",
    "  for word in text.split():\r\n",
    "    if word in misspelled_str:\r\n",
    "      corrected_str.append(spell.correction(word))\r\n",
    "    else:\r\n",
    "      corrected_str.append(word)\r\n",
    "  return \" \".join(corrected_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['text'] = train['text'].apply(lambda x : correct_spelling(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6cb11118430048aaffde37140829a96b2f2778371a079ecf7cef1d9eee249f62"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}